I"&b<blockquote>
  <p>“Yeah It’s leetcode problem. ”</p>
</blockquote>

<h2 id="聚类">聚类</h2>

<ol>
  <li>
    <p>关于K-means聚类算法，请回答以下问题：</p>

    <p>1) K-means是有监督聚类还是无监督聚类？（2分）</p>

    <p>2) 写出将N个样本（X=(x1,…,xN)）聚成K类的K-means聚类算法的优化目标函数。（6分）</p>

    <p>3) 请用伪代码写出聚类过程。（8分）</p>

    <p>4) 假设样本特征维度为D，请描述Kmeans算法时间复杂度。（4分）</p>

    <p><strong>答案：</strong></p>

    <p>1， 无监督</p>

    <p>2， 优化目标函数：F(X,K) =i=1Kj=1Ni(xj-μi)2</p>

    <p>​		对于样本xi，计算其分类：</p>

    <p><img src="https://www.nowcoder.com/equation?tex=c_%7Bi%7D%3Dargmin%7C%7Cx_%7Bi%7D-u_%7Bj%7D%7C%7C%5E%7B2%7D" alt="img" /></p>

    <p>​	   ci表示类别，uj表示质心</p>

    <p>​	   那么优化目标函数可以表示为：</p>

    <p><img src="https://www.nowcoder.com/equation?tex=%5Csum_%7Bi%7D%5E%7BN%7D%7B%7C%7Cx_%7Bi%7D-u_%7Bi%7D%7C%7C%5E%7B2%7D%7D" alt="img" /></p>

    <p>N表示样本量，ui表示ci的质心</p>

    <p>3， 聚类过程：</p>

    <p>​		初始化：从N个样本中随机选择K个作为初始聚类中心；</p>

    <p>​		For t=1:T（此处，T为最大迭代次数）</p>

    <p>​				将N个样本按距离最近原则分配给K个聚类中心；</p>

    <p>​				迭代更新聚类中心；</p>

    <p>​				如果达到终止条件，如全部样本归类无变化，或者样本点到聚类中心的平均距离变化率较低，则退出</p>

    <p>4， 时间复杂度：TNKD 其中T为迭代次数、N为样本个数，K为聚类中心数目，D为样本维度</p>
  </li>
</ol>

<h2 id="微积分">微积分</h2>

<h3 id="sgdmomentumadagardadam原理">SGD,Momentum,Adagard,Adam原理</h3>

<p><a href="https://www.cnblogs.com/jins-note/p/9520089.html">参考1</a> <a href="https://blog.csdn.net/willduan1/article/details/78070086">参考2</a> <a href="https://www.cnblogs.com/yifdu25/p/8183587.html">adam</a></p>

<p>SGD为随机梯度下降,每一次迭代计算数据集的mini-batch的梯度,然后对参数进行跟新。但是 优化中摆动幅度大，在最优点处容易震荡，要求学习率的设置需要始终，有一个很好的收敛速度同时又不至于摆动幅度太大。Momentum参考了物理中动量的概念,前几次的梯度也会参与到当前的计算中,但是前几轮的梯度叠加在当前计算中会有一定的衰减。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。让梯度的摆动幅度变得更小。</p>

<p>Adagard在训练的过程中可以自动变更学习的速率,设置一个全局的学习率,而实际的学习率与以往的参数模和的开方成反比。对于每个参数，随着其更新的总距离增多，其学习速率也随之变慢。即对于更新频繁步长大的参数，降低其学习率。<a href="https://www.jianshu.com/p/5c4784070d18">提出的原因</a>  daGrad也是为了解决鞍点和局部最优而出现的，是Rprop的一种改进。Rprop的缺点很明显，梯度容错率过低，如果存在一系列同号的梯度和突然的变号梯度，在所有同号梯度中，梯度会被削弱，而最后的变号梯度会被加强，如果变号梯度是由于计算错误导致的，那么这个错误将会被无限放大（特别是如果算出来的梯度本身就是很大的值的时候）。所以AdaGrad采用了累计平方梯度的思想，也就是用梯度自身的大小来约束梯度。起到的效果是在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。</p>

<p>在分母中累积平方梯度，导致学习率变小并变得无限小，由此导致还未收敛就已经停滞不前，出现early stopping 的现象。</p>

<p>RMSprop对AdaGrad做了一点改进，不再使用单纯的和累计，而是用了指数移动加权平均。这样做的好处就是，首先，可以通过调整<img src="https://math.jianshu.com/math?formula=%5Cbeta" alt="\beta" />来决定<img src="https://math.jianshu.com/math?formula=S%5Bt%5D" alt="S[t]" />对当前数据的敏感程度，其次由于指数移动平均加权就自带了正则化，所以<img src="https://math.jianshu.com/math?formula=S%5Bt%5D" alt="S[t]" />不会一直增大，而是会由加权窗口的数据平均决定，这样就很好地解决了问题。</p>

<p>Adam利用梯度的一阶矩估计(动量)和二阶矩（RMSProp）估计动态调整每个参数的学习率,在经过偏置的校正后,每一次迭代后的学习率都有个确定的范围,使得参数较为平稳。</p>

<p>由于移动指数平均在迭代开始的初期会导致和开始的值有较大的差异，所以我们需要对上面求得的几个值做偏差修正。</p>

<p>另外，在数据比较稀疏的时候，adaptive的方法能得到更好的效果，例如Adagrad，RMSprop, Adam 等。Adam 方法也会比 RMSprop方法收敛的结果要好一些<a href="https://www.jianshu.com/p/d99b83f4c1a6">参考</a>, 所以在实际应用中 ，Adam为最常用的方法，可以比较快地得到一个预估结果。</p>

<h2 id="朴素贝叶斯">朴素贝叶斯</h2>

<ol>
  <li>
    <p>最大似然估计和最大后验概率的区别?</p>

    <p>最大似然估计提供了一种给定观察数据来评估模型参数的方法,而最大似然估计中的采样满足所有采样都是独立同分布的假设。最大后验概率是根据经验数据获难以观察量的点估计,与最大似然估计最大的不同是最大后验概率融入了要估计量的先验分布在其中,所以最大后验概率可以看做规则化的最大似然估计。</p>
  </li>
  <li>
    <p>概率和似然的区别</p>

    <p>概率是指在给定参数<img src="https://uploadfiles.nowcoder.com/images/20190315/311436_1552620278389_BC90984DA33C4063A99AC4FA382A724D" alt="img" />的情况下,样本的随机向量X=x的可能性。而似然表示的是在给定样本X=x的情况下,参数<img src="https://uploadfiles.nowcoder.com/images/20190315/311436_1552620285666_30C645935F00A558745F9D014475B4AC" alt="img" />为真实值的可能性。一般情况,对随机变量的取值用概率表示。而在非贝叶斯统计的情况下,参数为一个实数而不是随机变量,一般用似然来表示。</p>
  </li>
</ol>

<h2 id="pca">PCA</h2>

<ol>
  <li>
    <p>讲一下PCA</p>

    <p>PCA是比较常见的线性降维方法,通过线性投影将高维数据映射到低维数据中,所期望的是在投影的维度上,新特征自身的方差尽量大,方差越大特征越有效,尽量使产生的新特征间的相关性越小。</p>

    <p>PCA算法的具体操作为对所有的样本进行中心化操作,计算样本的协方差矩阵,然后对协方差矩阵做特征值分解,取最大的n个特征值对应的特征向量构造投影矩阵。</p>
  </li>
</ol>

<h2 id="集成方法">集成方法</h2>

<p><a href="https://www.cnblogs.com/bnuvincent/p/9693190.html">GBDT</a></p>

<p><a href="https://www.cnblogs.com/bnuvincent/p/4905715.html">参考2</a></p>

<p><a href="https://www.jianshu.com/p/ac1c12f3fba1">XGBOOST</a></p>

<p><a href="https://blog.csdn.net/u010412858/article/details/80785429">stacking</a></p>

<p><a href="https://blog.csdn.net/sinat_35821976/article/details/83622594">blending</a></p>

<p><a href="https://www.biaodianfu.com/lightgbm.html">lightgbm</a> 码了再看</p>

<p><a href="https://www.cnblogs.com/peizhe123/p/5086128.html">Shrinkage</a></p>

<p>常规的机器学习算法问题，比如：XGB和GDBT相比有什么优势？[x先码</p>

<ul>
  <li>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li>
  <li>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。</li>
  <li>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</li>
  <li>
    <p>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</p>
  </li>
  <li>对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</li>
  <li>xgboost工具支持并行。</li>
</ul>

<p>LR、XGB、随机森林的原理、优缺点以及应用场景。这类问题几乎逢面必问，不光是腾讯，美团、头条等其它公司也是经常会问，建议大家好好准备一下这类问题；</p>

<p>参考1</p>

<p>LR和XGB算法做特征处理有什么区别？随机森林怎么进行特征选择？等特征处理方面相关的问题；</p>

<p>了解一般的机器学习吗，随机森林、xgboost、gbdt之间有什么区别和联系</p>

<h2 id="分类方法">分类方法</h2>

<h3 id="svm"><a href="https://blog.csdn.net/ch18328071580/article/details/94168411">SVM</a></h3>

<p>SVM是一种分类模型，首先，SVM是针对线性可分的情况进行分析的。对于线性不可分的情况，通过使用非线性映射算法将低维输入空间线性不可分的样本转化为高维特征空间，使其线性可分，从而使得在高维特征空间中采用线性算法对样本的非线性特征进行线性分析成为可能。
  它基于结构风险最小化理论，在特征空间中构建<strong>最优</strong>分类面，使得学习器能够得到全局最优化，并且使整个样本空间的期望风险以某个概率满足一定上界。</p>

<p>​	SVM没有使用传统的推导过程，简化了通常的分类和回归等问题；少数的支持向量确定了SVM 的最终决策函数，计算的复杂性取决于支持向量，而不是整个样本空间，这就可以避免“维数灾难”。少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本，而且注定了该方法不但算法简单，而且具有较好的“鲁棒”性。</p>

<p>首先，以线性可分数据为例，假设有m个样本，xi,yi, 其中呢 yi取值为1或-1，我们的目标是，找一个划分超平面，将不同类别的样本分开。划分的超平面很多，我们去找哪一个？直观上看，我们应该找位于两类训练样本“正中间”的超平面。划分超平面可通过如下线性方程来描述：<img src="/img/study/image-20200229115952823.png" alt="image-20200229115952823" />
<em>w</em>为法向量，决定了超平面的方向；b为位移项，决定了超平面与原点之间的距离。样本空间中任意点x到超平面(w,b)的距离可写为<img src="/img/study/image-20200229120033413.png" alt="image-20200229120033413" />假设超平面(w,b)能将训练样本正确分类，即对xiyi,若y=1,则 wx+b&gt;=1, 反之小于，距离超平面最近的这几个训练样本点使上式的等号成立，它们被称为“支持向量，所以两个相反的支持向量之间的间隔为2/||w||, 所以我们的目标就变成了，找到一组w,b，使得间隔最大。我们的上述约束等价于</p>

<p><img src="/img/study/image-20200229120656133.png" alt="image-20200229120656133" style="zoom:67%;" />，</p>

<p>这就是SVM的基本型。下面来求解模型，上式本身是一个凸二次规划问题，能直接用现成的优化计算包求解，但我们可以有更高效的办法，因为原问题的求解复杂度与特征的维数相关，而转成对偶问题后只与问题的变量个数有关。对式使用<strong>拉格朗日乘子法</strong>可得到其“<strong>对偶问题</strong>”(dual problem)：</p>

<p><img src="/img/study/image-20200229120807184.png" alt="image-20200229120807184" style="zoom:67%;" /></p>

<p>所以我们的目标就变成了，在alpha&gt;=0时，max(alpha) min（w,b）L。首先对x,b 求偏导数，</p>

<p><img src="/img/study/image-20200229120941989.png" alt="image-20200229120941989" style="zoom:50%;" /></p>

<p>将第一个式代入L，即可将L(w,b,α)中的w和b消去，再考虑第二个式的约束，就得到对偶问题:</p>

<p><img src="/img/study/image-20200229121018260.png" alt="image-20200229121018260" style="zoom:50%;" /></p>

<p>求解出对偶问题中的alpha（SMO算法，固定n-2个，先求两个）之后，就可以算出w, b（b的计算是因为对于所有的支撑向量，有y(wx+b)=1,w已知后，便可以求出b）。</p>

<p>从对偶问题解出的alpha_i是拉格朗日乘子，它恰对应着训练样本(xi,yi)。注意到支持向量机最优化问题中有不等式约束，因此上述过程需满足KKT条件，即要求:</p>

<p><img src="/img/study/image-20200229121543334.png" alt="image-20200229121543334" style="zoom:67%;" /></p>

<p>最后一个条件，对任意训练样本(xi,yi)总有αi=0 或y_{i}f(x_{i})=1</p>

<p>若αi=0则该样本将不会在求和中出现，不会对f(x)有任何影响;</p>

<p>若αi&gt;0,则y_{i}f(x_{i})=1位于最大间隔边界上,是一个支持向量。这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关.</p>

<p>在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分。
缓解该问题的一个办法是允许支持向量机在一-些样本上出错.为此，要引入“软间隔”的概念，在最大化间隔的同时，不满足约束的样本应尽可能少.于是,优化目标可写为:<img src="/img/study/image-20200229121844381.png" alt="image-20200229121844381" style="zoom:50%;" />即，在间隔上加一个损失，允许错分，但是损失应该尽量小。若采用hinge损失，则代价函数变成：<img src="/img/study/image-20200229122051192.png" alt="image-20200229122051192" style="zoom:50%;" /></p>

<p>这就是常用的“软间隔支持向量机”。</p>

<p>当数据是线性不可分的时候，对这样的问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。所以这个时候我们引入了一个映射函数h,原始公式中的x应替换为h(x), 这样替换之后，由于对偶问题中存在xT*x,就变成了求解<img src="/img/study/image-20200229122328923.png" alt="image-20200229122328923" style="zoom: 67%;" />，这是映射之后再特征空间中的内积，由于特征空间维数可能很高，甚至可能是无穷维，所以直接计算很困难，假设存在这样的函数<img src="/img/study/image-20200229122436268.png" alt="image-20200229122436268" style="zoom:67%;" />即xi 与xj 在特征空间的内积等于它们在原始样本空间中通过函数κ(∙,∙ )计算的结果。再次求解对偶函数可得到<img src="/img/study/image-20200229122533445.png" alt="image-20200229122533445" style="zoom:50%;" /></p>

<p>这里的函数κ(∙,∙ )就是“核函数”(kernel function)。显然，若已知合适映射ϕ(∙)的具体形式，则可写出核函数κ(∙,∙ )，但在现实任务中我们通常不知道ϕ(∙)是什么形式，所以，我们通常去寻找一个合适的核函数。核函数的定义是这样的，令χ为输入空间,κ(∙,∙ )是定义在χ×χ上的对称函数,则κ是核函数当且仅当对于任意数据D={x1,x2,…,xm}“核矩阵”(kernel matrix)K总是半正定的。在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，“核函数选择”成为支持向量机的最大变数。</p>

<p>通常，可选择如下核函数，选择性能最优者作为某一问题的核函数：，例如线性核，高斯核。（当样本的特征很多且维数很高时可考虑用SVM的线性核函数。当样本的数量较多,特征较少时,一般手动进行特征的组合再使用SVM的线性核函数。当样本维度不高且数量较少时,且不知道该用什么核函数时一般优先使用高斯核函数,因为高斯核函数为一种局部性较强的核函数,无论对于大样本还是小样本均有较好的性能且相对于多项式核函数有较少的参数。）</p>

<h3 id="svr">SVR</h3>

<p>给定训练样本D={(x1,y1),(x1,y1),……，(xm,ym)},yi∈R,希望学得一个回归模型,使得f(x)与y尽可能接近</p>

<p>假设我们能容忍f(x)与y之间最多有ϵ的偏差,即仅当f(x)与y之间的差别绝对值大于ϵ时才计算损失.</p>

<h2 id="基础知识">基础知识</h2>

<h3 id="交叉熵"><a href="https://blog.csdn.net/tsyccnh/article/details/79163834">交叉熵</a></h3>

<p>交叉熵基于KL散度计算两个分布之间的距离，将KL散度中的除法展开，可以展开成两项，</p>

<p><img src="/img/study/image-20200229114632787.png" alt="image-20200229114632787" style="zoom: 67%;" /></p>

<p>等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵：，p的熵即为真实分布的熵，是个常数项所以一般在机器学习中直接用用交叉熵做loss，评估模型。</p>

<p>为什么要用交叉熵做loss函数？<a href="https://zhuanlan.zhihu.com/p/84431551">参考</a></p>

<p>在线性回归问题中，常常使用MSE（Mean Squared Error）作为loss函数，如果直接将其扩展到分类问题上，使用sigmoid函数求得最终预测概率，首先 sigmoid函数在边缘处导数很小，很容易出现最开始的时候梯度下降很慢。其次由于对每个位置的特征都做了sigmoid，导致最终的loss函数很可能是非凸的，存在许多局部最优，然而使用log损失就不存在这种问题。MSE对残差大的样例惩罚更大些.比如真实标签分别是(1, 0, 0).模型1的预测标签是(0.8, 0.2, 0),模型2的是(0.9, 0.1, 0).即使输出的标签都是类别0, 但MSE-based算出来模型1的误差是模型2的4倍,而交叉熵-based算出来模型1的误差是模型2的2倍左右.为了弥补模型1在这个样例上的损失,MSE-based需要3个完美预测的样例才能达到和模型2一样的损失,而交叉熵-based只需要一个.实际上,模型输出正确的类别,0.8可能已经是个不错的概率了.</p>

<p>softmax、多个logistic的各自的优势：1、类别数爆炸，2、推了下softmax反向传播的公式，来对比两者的优劣。</p>

<h3 id="过拟合">[过拟合]</h3>

<ol>
  <li>
    <p>为什么会发生过拟合</p>

    <p>（1）建模样本抽取错误，包括（但不限于）样本数量太少，抽样方法错误，抽样时没有足够正确考虑业务场景或业务特点，等等导致抽出的样本数据不能有效足够代表业务逻辑或业务场景；
（2）样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系；
（3）建模时的“逻辑假设”到了模型应用时已经不能成立了。任何预测模型都是在假设的基础上才可以搭建和应用的，常用的假设包括：假设历史数据可以推测未来，假设业务环节没有发生显著变化，假设建模数据与后来的应用数据是相似的，等等。如果上述假设违反了业务场景的话，根据这些假设搭建的模型当然是无法有效应用的。
（4）参数太多、模型复杂度高
（5）决策树模型。如果我们对于决策树的生长没有合理的限制和修剪的话，决策树的自由生长有可能每片叶子里只包含单纯的事件数据(event)或非事件数据（no event），可以想象，这种决策树当然可以完美匹配（拟合）训练数据，但是一旦应用到新的业务真实数据时，效果是一塌糊涂。
（6）神经网络模型。
a.由于对样本数据,可能存在隐单元的表示不唯一,即产生的分类的决策面不唯一.随着学习的进行, BP算法使权值可能收敛过于复杂的决策面,并至极致.
b.权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征.</p>
  </li>
  <li>
    <p>那如何降低过拟合？</p>

    <p><strong>权值衰减. 主要应用在神经网络模型中</strong>
它在每次迭代过程中以某个小因子降低每个权值,这等效于修改E的定义,加入一个与网络权值的总量相应的
惩罚项,此方法的动机是保持权值较小,避免weight decay,从而使学习过程向着复杂决策面的反方向偏</p>

    <p><strong>验证数据</strong>
一个最成功的方法是在训练数据外再为算法提供一套验证数据,应该使用在验证集合上产生最小误差
的迭代次数,不是总能明显地确定验证集合何时达到最小误差.</p>

    <p>交叉验证：<strong>交叉验证帮助我们确定模型的超参数</strong>。这样训练出来的模型，是经过多次综合比较得出的相对最优模型，在一定程度上可以避免过拟合的问题，这就是为什么我们会说交叉验证可以避免过拟合。</p>

    <p>降低模型复杂度、</p>

    <p>增大数据集、重新采样（改变采样方法等）、重新清洗数据、重新筛选特征。</p>

    <p>dropout、</p>

    <p>正则化、</p>

    <p>集成模型、</p>

    <p>BN</p>

    <p>soft target?</p>
  </li>
  <li>
    <p>讲下正则化</p>
  </li>
</ol>

<p>讲了L0\L1\L2，L1和L2的具体形式和反向传播的时梯度的推导公式、以及它们如何降低过拟合都详细说了。</p>

<p>https://blog.csdn.net/qq_16137569/article/details/81584165</p>

<p>https://www.jianshu.com/p/4bad38fe07e6</p>

<ol>
  <li>泛化误差如何产生，有哪些方法可以减小？</li>
</ol>

<p>讲了泛化误差的展开式，西瓜书45页，分别说了从bias、var和噪声三个方面减小，并说了相关的方式，以及如何平衡bias和var。</p>

<ol>
  <li>
    <p>讲下dropout原理。</p>

    <p>Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。</p>
  </li>
  <li>
    <p>dropout训练和测试有什么区别吗？</p>

    <p>https://www.cnblogs.com/ziytong/p/10669239.html</p>

    <p>经过上面屏蔽掉某些神经元，使其激活值为0以后，我们还需要对向量y1……y1000进行缩放，也就是乘以1/(1-p)。如果你在训练的时候，经过置0后，没有对y1……y1000进行缩放（rescale），那么在测试的时候，就需要对权重进行缩放，操作如下。在测试模型阶段   预测模型的时候，每一个神经单元的权重参数要乘以概率p。</p>
  </li>
  <li>
    <p>L1不可导的时候该怎么办<a href="https://blog.csdn.net/pxhdky/article/details/82960659">参考</a></p>

    <p>当损失函数不可导,梯度下降不再有效,可以使用坐标轴下降法,梯度下降是沿着当前点的负梯度方向进行参数更新,而坐标轴下降法是沿着坐标轴的方向,假设有m个特征个数,坐标轴下降法进参数更新的时候,先固定m-1个值,然后再求另外一个的局部最优解,从而避免损失函数不可导问题。</p>

    <p>使用Proximal Algorithm近端梯度下降法对L1进行求解,此方法是去优化损失函数上界结果。</p>

    <p>另外 为何L1容易产生稀疏特征，<a href="https://www.cnblogs.com/tianqizhi/p/9703796.html">参考</a></p>
  </li>
  <li>
    <p>LASSO回归 <a href="https://blog.csdn.net/pxhdky/article/details/82960659">参考</a></p>
  </li>
</ol>

<h2 id="python">python</h2>

<ol>
  <li>多线程和多进程是什么？</li>
  <li>进程内存互相访问</li>
  <li>python内存管理，内存池最大？</li>
  <li>如何对一段python代码做加速</li>
  <li>讲下python的staticmethod和staticclass：没听过，但面试官谈到了装饰器，我就介绍了装饰器相关的。</li>
  <li>讲下python的匿名函数和意义</li>
  <li>对浅拷贝和深拷贝有了解吗，讲一下。</li>
  <li>python里的正则表达式</li>
  <li>python可变不可变数据结构。</li>
  <li>python lamba与def 定义函数的区别</li>
  <li>实现opencv中的图像缩放，包括实现双线性插值</li>
</ol>

<p>​</p>

:ET