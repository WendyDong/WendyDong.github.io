---
layout:     post
title:      " CV problems "
subtitle:   " CV problems "
date:       2020-12-04 17:09:00
author:     "DHH"
header-img: "img/bg/leetcodebg.jpg"
catalog: true
tags:
    - 笔试
    - 计算机视觉
typora-root-url: ..\..
---

> “Yeah It's leetcode problem. ”

## 传统图像处理

### 	[sift](https://blog.csdn.net/zddblog/article/details/7521424)

生成高斯差分金字塔,尺度空间构建,（使用高斯差分金字塔近似LoG算子）

空间极值点检测（中间的检测点和它同尺度的8个相邻点和上下相邻尺度对应的9×2个点共26个点比较，以确保在尺度空间和二维图像空间都检测到极值点。）,

稳定关键点的精确定位, （去除低对比度的关键点和不稳定的边缘响应点，利用已知的离散空间点插值得到的连续空间极值点的方法叫做子像素插值）

稳定关键点方向信息分配（,使用直方图统计邻域内像素的梯度和方向，直方图的峰值方向代表了关键点的主方向

关键点描述（SIFT描述子是关键点邻域高斯图像梯度统计结果的一种表示。通过对关键点周围图像区域分块，计算块内梯度直方图，生成具有独特性的向量，这个向量是该区域图像信息的一种抽象，具有唯一性。）

特征点匹配。

### **色彩迁移** [参考](http://www.mamicode.com/info-detail-2301867.html)

RGB三通道有很强的关联性，而做颜色的改变同时恰当地改变三通道比较困难。

需要寻找三通道互不相关的也就是正交的颜色空间，作者想到了Ruderman等人提出的lαβ颜色空间。三个轴向正交意味着改变任何一个通道都不影响其他通道，从而能够较好的保持原图的自然效果。三个通道分别代表：亮度，黄蓝通道，红绿通道。

要操作的图片称作源图片（source），想要转成的颜色风格成为目标图片（target）。

分通道计算源图片的均值和标准差，分通道计算目标图片的均值和标准差。

源图片逐像素减均值（减去对应通道的均值）： 乘目标图片与源图片标准差的比值：加上目标图片的均值。

整个过程可以简单描述为：分通道计算，减去自己的均值，除以自己的方差，乘目标的方差，加目标的均值。

### 	[surf](https://blog.csdn.net/blateyang/article/details/76512398)

### 	cdvs: 

兴趣点检测、局部特征选择、局部特征描述、局部特征描述子聚合、局部特征描述子压缩、局部特征位置压缩

局部特征选择: 首先按照兴趣点的LoG卷积响应值和主曲率比率的概率值从大到小排序，根据排序结果筛选出前2*N个候选兴趣点；然后按照兴趣点的尺度因子的概率值进一步排序，筛选1.5N个兴趣点；最后按照兴趣点的坐标编号从小到大排序，筛选前N个兴趣点。

局部特征描述子聚合:在全局特征聚合之前，所有用于聚合的128维的局部描述子先被归一化并进行PCA降维，得到32维的降维向量。降维结束后，采用基于Fisher向量聚合方法对上述向量样本进行聚合。[Fisher向量聚合](https://www.cnblogs.com/jie-dcai/p/5740480.html)是基于一个含有512个高斯分布函数的高斯混合模型（Gaussian Mixture Model， [GMM](https://blog.csdn.net/happyer88/article/details/46576379)） 在全局描述子聚合阶段，若干高斯分布函数将被选择用于聚合。具体的高斯函数选取方法如下：首先，高斯分布函数根据均值累计梯度向量标准差的取值降序排序；随后，对于操作点限制在码流长度为512字节、1024字节及2048字节的描述子，排序靠前的k个高斯分布函数将被用于后续全局特征码流生成。选取大于某个阈值的高斯函数。选择高斯函数后，对相应的梯度向量每一维进行二值化

## WGAN）

[参考](https://zhuanlan.zhihu.com/p/25071913) GAN 要会手写loss cycle gan wGAN

做法：

- 判别器最后一层去掉sigmoid (把分类变成了回归)

- 生成器和判别器的loss不取log

- 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c

- 不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行 （梯度本身就是不稳定的）

好处：

- 彻底解决GAN训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度

- 基本解决了collapse mode的问题，确保了生成样本的多样性

- 训练过程中终于有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表GAN训练得越好，代表生成器产生的图像质量越高（如题图所示）

- 以上一切好处不需要精心设计的网络架构，最简单的多层全连接网络就可以做到

原因：

1. 原始GAN的第一种loss

根据原始GAN定义的判别器loss，我们可以得到最优判别器的形式；而在最优判别器的下，我们可以把原始GAN定义的生成器loss等价变换为最小化真实分布![[公式]](https://www.zhihu.com/equation?tex=P_r)与生成分布![[公式]](https://www.zhihu.com/equation?tex=P_g)之间的JS散度。通过分析JS散度，对于最优判别器，无论![[公式]](https://www.zhihu.com/equation?tex=P_r)跟![[公式]](https://www.zhihu.com/equation?tex=P_g%0A)是远在天边，还是近在眼前，只要它们俩没有一点重叠或者重叠部分可忽略，JS散度就固定是常数![[公式]](https://www.zhihu.com/equation?tex=%5Clog+2)，**而这对于梯度下降方法意味着——梯度为0**！此时对于最优判别器来说，生成器肯定是得不到一丁点梯度信息的；即使对于接近最优的判别器来说，生成器也有很大机会面临梯度消失的问题。

有了这些理论分析，原始GAN不稳定的原因就彻底清楚了：判别器训练得太好，生成器梯度消失，生成器loss降不下去；判别器训练得不好，生成器梯度不准，四处乱跑。只有判别器训练得不好不坏才行，但是这个火候又很难把握，甚至在同一轮训练的前后不同阶段这个火候都可能不一样，所以GAN才那么难训练。

（解决方法：对生成样本和真实样本加噪声，但是 不能成为![[公式]](https://www.zhihu.com/equation?tex=P_r)和![[公式]](https://www.zhihu.com/equation?tex=P_g)距离的本质性衡量）

2. 原始GAN的第二种loss:**collapse mode**

因为KL散度不是一个对称的衡量，![[公式]](https://www.zhihu.com/equation?tex=KL%28P_g+%7C%7C+P_r%29)与![[公式]](https://www.zhihu.com/equation?tex=KL%28P_r+%7C%7C+P_g%29)是有差别的。以前者为例，![[公式]](https://www.zhihu.com/equation?tex=KL%28P_g+%7C%7C+P_r%29)对于上面两种错误的惩罚是不一样的，第一种错误对应的是“生成器没能生成真实的样本”，惩罚微小；第二种错误对应的是“生成器生成了不真实的样本” ，惩罚巨大。第一种错误对应的是缺乏多样性，第二种错误对应的是缺乏准确性。

3. 解决：Wasserstein距离的优越性质

Wasserstein距离又叫Earth-Mover（EM）距离，定义如下：

![[公式]](https://www.zhihu.com/equation?tex=W%28P_r%2C+P_g%29+%3D+%5Cinf_%7B%5Cgamma+%5Csim+%5CPi+%28P_r%2C+P_g%29%7D+%5Cmathbb%7BE%7D_%7B%28x%2C+y%29+%5Csim+%5Cgamma%7D+%5B%7C%7Cx+-+y%7C%7C%5D)（公式12）

Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近。

![[公式]](https://www.zhihu.com/equation?tex=W%28P_r%2C+P_g%29+%3D+%5Cfrac%7B1%7D%7BK%7D+%5Csup_%7B%7C%7Cf%7C%7C_L+%5Cleq+K%7D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_r%7D+%5Bf%28x%29%5D+-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5Bf%28x%29%5D)（公式13）

![[公式]](https://www.zhihu.com/equation?tex=%7Cf%28x_1%29+-+f%28x_2%29%7C+%5Cleq+K+%7Cx_1+-+x_2%7C)

![[公式]](https://www.zhihu.com/equation?tex=K+%5Ccdot+W%28P_r%2C+P_g%29+%5Capprox+%5Cmax_%7Bw%3A+%7Cf_w%7C_L+%5Cleq+K%7D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_r%7D+%5Bf_w%28x%29%5D+-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5Bf_w%28x%29%5D)（公式14）

最后，还不能忘了满足公式14中![[公式]](https://www.zhihu.com/equation?tex=%7C%7Cf_w%7C%7C_L+%5Cleq+K)这个限制。我们其实不关心具体的K是多少，只要它不是正无穷就行，因为它只是会使得梯度变大![[公式]](https://www.zhihu.com/equation?tex=K)倍，并不会影响梯度的方向。所以作者采取了一个非常简单的做法，就是限制神经网络![[公式]](https://www.zhihu.com/equation?tex=f_%5Ctheta)的所有参数![[公式]](https://www.zhihu.com/equation?tex=w_i)的不超过某个范围![[公式]](https://www.zhihu.com/equation?tex=%5B-c%2C+c%5D)。

注意原始GAN的判别器做的是真假二分类任务，所以最后一层是sigmoid，但是现在WGAN中的判别器![[公式]](https://www.zhihu.com/equation?tex=f_w)做的是近似拟合Wasserstein距离，属于回归任务，所以要把最后一层的sigmoid拿掉。


## 模型压缩/知识[蒸馏](如何理解soft target这一做法？ - YJango的回答 - 知乎 https://www.zhihu.com/question/50519680/answer/136406661)

  这种方法感觉受到了ensemble的启发，利用大型（teacher net）网络提取先验知识，将这种先验知识作为soft target让微型网络（student network）学习,有点像Boost中第一个分类器学到后调整weight让第二个分类器学习。当然相似中也有不同之处。

​	通过复杂网络产生的分类概率分布作为**soft target**来训练小模型。

​	加入蒸馏后的softmax函数：	![image-20200304175316267](/img/study/image-20200304175316267.png)

​	这里T是超参数，文中说是‘温度’，经过该参数之后的softmax会更加平滑（T越大越平滑），分布更加均匀而大小关系不变。T参数在设置为1的时候就是平常的softmax函数。
 **在知识转换阶段，设置复杂网络与简易网络相同的T参数。在此之后再从新将T设置为1**

![具体过程](https://img-blog.csdn.net/20180306210740358?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI3NDk2OTk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 

1、训练大模型：先用hard target，也就是正常的label训练大模型。
2、计算soft target：利用训练好的大模型来计算soft target。也就是大模型“软化后”再经过softmax的output。
3、训练小模型，在小模型的基础上再加一个额外的soft target的loss function，通过lambda来调节两个loss functions的比重。在做梯度下降时应该将soft target的梯度乘以T^2。
4、预测时，将训练好的小模型按常规方式（右图）使用。



soft target的理解：

1 可以使用teacher model对于每个样本输出一个连续的label分布，这样可以利用的监督信息就远比one hot的多了。可以想象如果只有label这样的一个目标的话，那么这个模型的目标就是把训练样本中每一类的样本强制映射到同一个点上，这样其实对于训练很有帮助的类内variance和类间distance就损失掉了。然而使用teacher model的输出可以恢复出这方面的信息。

**2 soft target是加入了关于想要拟合的mapping的prior knowledge，降低了神经网络的搜索空间，从而获得了泛化（generalization）能力。**

## 目标检测([参考](https://baijiahao.baidu.com/s?id=1598999301741831102&wfr=spider&for=pc))

1. 目前通过卷积神经网络进行检测的方法主要分为one-stage和two-stage，分别写出了解的对应的算法。

   在共性上两类检测算法有哪些差异？

   1)One-stage：yolov1、yolov2、yolov3、SSD、RetinaNet

    Two-stage：Fast R-CNN、Faster R-CNN

   2)Two-stage检测算法的共性，以faster r-cnn为例，使用了复杂的网络用于每个候选区域的分类和回归；ROI pooling后的feature channels数目较大，导致内存消耗和计算量都比较大。

   One-stage检测算法的共性，从网络结构上看只是多分类的rpn网络，相当于faster rcnn的第一阶段，因此one-stage主要的优势是速度快。其预测结果是从feature map回归出目标的位置及分类，有的也采用了anchor的概念。而two-stage对上述结果进行roi pooling后会进一步细化，因此two-stage算法检测精度一般相对较高。还有一种观点是，two-stage的rpn部分相当于做了正负样本均衡，这也是two-stage检测效果相对较好的一个原因。one-stage算法对小目标检测效果较差，如果所有的anchor都没有覆盖到这个目标，那么这个目标就会漏检。如果一个比较大的anchor覆盖了这个目标，那么较大的感受野会弱化目标的真实特征，得分也不会高。two-stage算法中的roi pooling会对目标做resize, 小目标的特征被放大，其特征轮廓也更为清晰，因此检测也更为准确。
   
2. R-CNN

   1. 输入一张图片，通过指定算法从图片中提取 2000 个类别独立的候选区域（可能目标区域）SS（slective search） 很耗时
   2. 对于每个候选区域利用卷积神经网络来获取一个特征向量 耗时
   3. 对于每个区域相应的特征向量，利用支持向量机SVM 进行分类，并通过一个bounding box regression调整目标包围框的大小。三个东西分别训练 耗时
   
3. Fast-RCNN

   1. 首先还是采用selective search提取2000个候选框RoI

   2. 使用一个卷积神经网络对全图进行特征提取

   3. 使用一个RoI Pooling Layer在全图特征上摘取每一个RoI对应的特征

   4. 分别经过为21和84维的全连接层（并列的，前者是分类输出，后者是回归输出）

     总结： Fast R-CNN通过CNN直接获取整张图像的特征图，再使用RoI Pooling Layer在特征图上获取对应每个候选框的特征，避免了R-CNN中的对每个候选框串行进行卷积（耗时较长）。但是SS这个过程还是很耗时

4. Faster-RCNN

   Faster R-CNN 取代selective search，直接通过一个Region Proposal Network (RPN)生成待检测区域，这么做，在生成RoI区域的时候，时间也就从2s缩减到了10ms。下图是Faster R-CNN整体结构。

   ![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hZTAxLmFsaWNkbi5jb20va2YvVVRCOHRWNzJPd1F5ZGVKazQzUFVxNkF5UXBYYWsuanBn)


   由上图可知，Faster R-CNN由共享卷积层、RPN、RoI pooling以及分类和回归四部分组成：

   首先使用共享卷积层为全图提取特征feature maps
   将得到的feature maps送入RPN，RPN生成待检测框(指定RoI的位置),并对RoI的包围框进行第一次修正
   RoI Pooling Layer根据RPN的输出在feature map上面选取每个RoI对应的特征，并将维度置为定值
   使用全连接层(FC Layer)对框进行分类，并且进行目标包围框的第二次修正。
   尤其注意的是，Faster R-CNN真正实现了端到端的训练(end-to-end training)。

   Faster R-CNN中的RoI Pooling Layer与 Fast R-CNN中原理一样。在RoI Pooling Layer之后，就是Faster R-CNN的分类器和RoI边框修正训练。分类器主要是分这个提取的RoI具体是什么类别(人，车，马等)，一共C+1类(包含一类背景)。RoI边框修正和RPN中的anchor边框修正原理一样，同样也是[SmoothL1 Loss](https://blog.csdn.net/yang_daxia/article/details/91360606)。

5. MaskRCNN

   Mask R-CNN可以分解为如下的3个模块：Faster-RCNN、RoI Align和Mask。算法框架如下：

   ![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hZTAxLmFsaWNkbi5jb20va2YvVVRCOFgyTXp2dXZKWEtKa1Nhamg3NjM3YUZYYTAucG5n)

   RoI Pooling和RoIAlign最大的区别是：前者使用了两次量化操作，而后者并没有采用量化操作，使用了线性插值算法，具体的解释如下所示。(利用利用双线性插值来估计这些蓝点，最后求max)

6. ![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9hZTAxLmFsaWNkbi5jb20va2YvVVRCOGlxcGhPSm9TZGVKazQzT3c3NjFhNFhYYVgucG5n)

   Yolo算法开创了one-stage检测的先河，它将物体分类和物体检测网络合二为一，都在全连接层完成。故它大大降低了目标检测的耗时，提高了实时性。但它的缺点也十分明显。

   每个网格只对应两个bounding box，当物体的长宽比不常见（也就是训练数据集覆盖不到时），效果很差。
   原始图片只划分为7x7的网格，当两个物体靠的很近时，效果很差
   最终每个网格只对应一个类别，容易出现漏检（物体没有被识别到）。
   对于图片中比较小的物体，效果很差。这其实是所有目标检测算法的通病，SSD对它有些优化。

7. SSD

   SSD和Yolo一样都是采用一个CNN网络来进行检测，但是却采用了多尺度的特征图.

   SSD认为目标检测中的物体，只与周围信息相关，它的感受野不是全局的，故没必要也不应该做全连接。SSD的特点如下。

   1、 每一个卷积层，都会输出不同大小感受野的feature map。在这些不同尺度的feature map上，进行目标位置和类别的训练和预测，从而达到多尺度检测的目的，可以克服yolo对于宽高比不常见的物体，识别准确率较低的问题。

   2、 在Yolo中，每个单元预测多个边界框，但是其都是相对这个单元本身（正方块），但是真实目标的形状是多变的，Yolo需要在训练过程中自适应目标的形状。而SSD和Faster R-CNN相似，也提出了anchor的概念。卷积输出的feature map，每个点对应为原图的一个区域的中心点。以这个点为中心，构造出6个宽高比例不同，大小不同的anchor（SSD中称为default box）。每个anchor对应4个位置参数(x,y,w,h)和21个类别概率（voc训练集为20分类问题，在加上anchor是否为背景，共21分类）。SSD的检测值也与Yolo不太一样。对于每个单元的每个先验框，其都输出一套独立的检测值，对应一个边界框。

   另外，SSD采用了数据增强。生成与目标物体真实box间IOU为0.1 0.3 0.5 0.7 0.9的patch，随机选取这些patch参与训练，并对他们进行随机水平翻转等操作


## [faiss](https://blog.csdn.net/kanbuqinghuanyizhang/article/details/80774609) 大规模检索

1、倒排索引：这里的索引是聚类中心。如果需要存储的向量太多，通过暴力搜索索引`IndexFlatL2`速度很慢，这里介绍一种加速搜索的方法的索引`IndexIVFFlat`。翻译过来叫倒排文件，其实是使用K-means建立聚类中心，然后通过查询最近的聚类中心，然后比较聚类中的所有向量得到相似的向量。

2、快速的KNN、[ANN](https://yongyuan.name/blog/ann-search.html)算法

### [乘积量化PQ](https://blog.csdn.net/chenyq991/article/details/78934989?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task)

### [IVFOPQ](https://yongyuan.name/blog/ann-search.html)

KDTree，LSH，ITQ

倒排PQ乘积量化(IVFPQ)



## 不均衡样本

### [focal loss](https://blog.csdn.net/qwer7512090/article/details/93136325)

![image-20200305131202471](/img/study/image-20200305131202471.png)

上下采样

提前困难样本挖掘

样本不均衡怎么处理？一个batch类别均等采样，修改loss对不同样本的权重。

## 基础知识

### BP

DNN反向传播公式推导

CNN反向传播公式推导

矩阵正定性的判断,Hessian矩阵正定性在梯度下降中的应用

若矩阵所有特征值均不小于0,则判定为半正定。若矩阵所有特征值均大于0,则判定为正定。在判断优化算法的可行性时Hessian矩阵的正定性起到了很大的作用,若Hessian正定,则函数的二阶偏导恒大于0,函数的变化率处于递增状态,在牛顿法等梯度下降的方法中,Hessian矩阵的正定性可以很容易的判断函数是否可收敛到局部或全局最优解。

[拟牛顿法](https://zhuanlan.zhihu.com/p/46536960)的[原理](https://blog.csdn.net/itplus/article/details/21896453)

牛顿法的收敛速度快,迭代次数少,但是Hessian矩阵很稠密时,每次迭代的计算量很大,随着数据规模增大,Hessian矩阵也会变大,需要更多的存储空间以及计算量。拟牛顿法就是在牛顿法的基础上引入了Hessian矩阵的近似矩阵,避免了每次都计算Hessian矩阵的逆,在拟牛顿法中,用Hessian矩阵的逆矩阵来代替Hessian矩阵,虽然不能像牛顿法那样保证最优化的方向,但其逆矩阵始终是正定的,因此算法始终朝最优化的方向搜索。

### 1*1卷积核好处

专注于跨通道的特征组合；对feature map的channel级别降维或升维

### [感受野](https://blog.csdn.net/program_developer/article/details/80958716) 

定义：一个卷积核可以映射原始输入图的区域大小。 计算：![image-20200225101050685](/img/study/image-20200225101050685.png)

### tensorflow [conv2d](https://blog.csdn.net/g0415shenw/article/details/86081330)

讲下tensorflow搭建网络和训练的流程。

### pytorch [conv2d](https://ptorch.com/docs/1/torch-nn)

### [BN](https://www.cnblogs.com/eilearn/p/9780696.html) 手写



首先，BN是用来解决“Internal Covariate Shift” 即如果ML系统实例集合<X,Y>中的输入值X的分布老是变，这不符合IID假设，网络模型很难稳定的学规律，所以我们在输入层会进行归一化，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，即Internal Covariate Shift。所以能不能让每个隐层节点的激活输入分布固定下来呢？这样就避免了“Internal Covariate Shift”问题了，顺带解决反向传播中梯度消失问题。BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，**这样使得激活输入值落在非线性函数对输入比较敏感的区域，**使得梯度变大，加快训练速度。具体计算是在一个minibatch内，将某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)。但是如果把分布全部拉回了线性区域，去少了非线性变换，网络的表征能力会大大下降，所以BN**为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale\*x+shift)**，通过网络自身的学习得到这两个参数，使得分布稍微遍历中心的线性区域。

在预测阶段，**可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量**，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。

优点：提升了训练速度，收敛过程大大加快；还能增加分类效果，提高网络的泛化能力(感觉是因为，即使训练数据与测试数据分布不同，也会通过bn将分布映射到相同的位置上)。（一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；另外调参过程也简单多了），对于初始化要求没那么高，而且可以使用大的学习率等。

### [one-hot](https://blog.csdn.net/Li_yi_chao/article/details/80852701) 

介绍one-hot，为什么采用one-hot

解决了分类器不好处理属性数据的问题，让特征之间的距离计算更加合理;在一定程度上也起到了扩充特征的作用，比如性别本身是一个特征，经过one hot编码以后，就变成了男或女两个特征;将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。

One-hot是对低维高信息量的特征在高维中进行打散，使之在模型中更容易被优化/学习(拟合难度降低)



### pooling意义

增大深层卷积的感受野。降维减少参数量和减少卷积后的冗余。

具有局部平移不变形，提高网络效率

### [inception](https://baijiahao.baidu.com/s?id=1601882944953788623&wfr=spider&for=pc) 

**inceptionV1**

![image-20200305133138163](/img/study/image-20200305133138163.png)左边的是一般的版本，右边的是改进的版本

**v2**

变化1：(引入了BN层，把5x5的换成两个3x3的)

变化2：（使用两个1*n和n*1的卷积核取代n*n的卷积核。这样做，相比把5*5用两个3*3代替更加参数量少，加速了网络的变化。同时增加了一层非线性扩展模型表达能力。论文解释说，这种非对称的卷积结构拆分，其结果比对称地拆为几个相同的小卷积核效果更明显，可以处理更多、更丰富的空间特征，增加特征多样性;

变化3： 模块中的滤波器组被扩展（即变得更宽而不是更深），以解决表征性瓶颈。如果该模块没有被拓展宽度，而是变得更深，那么维度会过多减少，造成信息损失。如下图所示：

**v3:**整合了前面 Inception v2 中提到的所有升级,RMSProp 优化器；Factorized 7x7 卷积；辅助分类器使用了 BatchNorm；标签平滑（添加到损失公式的一种正则化项，旨在阻止网络对某一类别过分自信，即阻止过拟合）。

**v4** 使用了不同的 Inception Block,然后堆叠成Inception V4

Inception-Resnet 结合了resnet的残差。

mobileNet、shufflenet的原理？说了下原理。 		

​				为什么mobileNet在理论上速度很快，工程上并没有特别大的提升？ 先说了卷积源码上的实现，两个超大矩阵相乘，可能是group操作，是一些零散的卷积操作，速度会慢。（大佬觉得不满意，说应该从内存上去考虑。申请空间？确实不太清楚。）

### [VGG](https://blog.csdn.net/jyy555555/article/details/80515562)

1. **小卷积核。**作者将卷积核全部替换为3x3（极少用了1x1）；更多的激活函数、更丰富的特征，更强的辨别能力。卷积层的参数减少

2. **小池化核。**相比AlexNet的3x3的池化核，VGG全部为2x2的池化核；在图像任务上max-pooling的效果更胜一筹，所以图像大多使用max-pooling。在这里我认为max-pooling更容易捕捉图像上的变化，梯度的变化，带来更大的局部信息差异性，更好地描述边缘、纹理等构成语义的细节信息，这点尤其体现在网络可视化上。

3. **层数更深特征图更宽。**基于前两点外，由于卷积核专注于扩大通道数、池化专注于缩小宽和高，使得模型架构上更深更宽的同时，计算量的增加放缓；

4. **全连接转卷积。**网络测试阶段将训练阶段的三个全连接替换为三个卷积，测试重用训练时的参数，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高为的输入。在测试阶段把网络中原本的三个全连接层依次变为1个conv7x7，2个conv1x1，

### [xception](https://blog.csdn.net/lk3030/article/details/84847879)  [mobilenet](https://www.cnblogs.com/dengshunge/p/11334640.html) 与xception[区别](https://blog.csdn.net/u013548568/article/details/79910669) 

### Flops计算：

计算flops，卷积维度变换的公式推导，卷积是如何编程实现的；

<img src="/img/study/image-20200225121221770.png" alt="image-20200225121221770" style="zoom: 67%;" /> 不考虑bias时有-1，有bias时没有-1。

输出大小计算：

 ![image-20200225121334638](/img/study/image-20200225121334638.png) 

参数量 `F*F*Cin*Cout`

9. 默写交叉熵和softmax，还有它的[BP](https://blog.csdn.net/sunlanchang/article/details/88825134) 
10. [PCA](https://blog.csdn.net/program_developer/article/details/80632779) 

### 激活函数相关

1. sigmoid函数特性

定义域为![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552619092213_171B20A604B88BE2CC570EF936359F57)

值域为(0,1)

函数在定义域内为连续和光滑的函数

处处可导,导数为![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552619110289_2FC0EF550E43398C033CAF8AE5CD23F6)

2. 讲下激活函数的意义

   增大非线性表征能力，充分组合特征。

3. 为什么要用relu而不用sigmoid

   从饱和区间、敏感区间和梯度三个方面来回答。

   第一，采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。 
   第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），从而无法完成深层网络的训练。 
   第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。

4. softplus

   softplus可以看作是ReLu的平滑y=log(1+ex)

5. 梯度消失 爆炸

   https://www.cnblogs.com/makefile/p/activation-function.html

   解决方法

   权重初始化
   使用合适的方式初始化权重, 如ReLU使用MSRA的[初始化方式](https://www.cnblogs.com/wangguchangqing/p/11013698.html), tanh使用xavier初始化方式.

   激活函数选择
   激活函数要选择ReLU等梯度累乘稳定的.

   学习率
   一种训练优化方式是对输入做白化操作(包括正规化和去相关), 目的是可以选择更大的学习率. 现代深度学习网络中常使用Batch Normalization(包括正规化步骤,但不含去相关).

### 卷积相关

卷积的实现，意义

卷积运算通过三个重要的思想来帮助改进机器学习系统：稀疏交互、参数共享、等变表示

**1\*1卷积过滤器** ,它的大小是**1\*1**，没有考虑在前一层局部信息之间的关系。最早出现在 Network In Network的论文中 ，使用1*1卷积是想加深加宽网络结构 ，在Inception网络（ Going Deeper with Convolutions ）中用来降维。还可以增加非线性和减少参数

由于3*3卷积或者5*5卷积在几百个filter的卷积层上做卷积操作时相当耗时，所以1*1卷积在3*3卷积或者5*5卷积计算之前先降低维度。



## 还需要看的

混合高斯模型

yolov2、yolov3、RetinaNet

图像预处理

1. 编码实现计算图的inference （用Java基本完整写出来了）



1. 手写一个卷积（我用的Numpy..）

2. partial conv原理

3. .l1 loss,l2 loss,smooth l1 loss区别

4. BN层原理，测试和训练的区别，测试怎么得到mean和std（滑动平均）

5. 原地swap(a,b) 异或？

6. C++虚函数

7. c++深拷贝，浅拷贝，赋值的区别；c++虚函数的作用；c++指针求sizeof得到什么，以及32位机和64位机的size区别

   指针变量本身的大小，此处是一个指针变量，指针变量永远为32位4字节

8. C++堆和栈的区别

   堆是不连续的内存区域（因为系统是用链表来存储空闲内存地址，自然不是连续的），堆大小受限于计算机系统中有效的虚拟内存（32bit  系统理论上是4G），所以堆的空间比较灵活，比较大。栈是一块连续的内存区域，大小是操作系统预定好的，windows下栈大小是2M（也有是1M，在  编译时确定，VC中可设置）。
   堆向上，向高地址方向增长。栈向下，向低地址方向增长。

   堆中资源由程序员控制（容易产生memory leak）， 栈资源由编译器自动管理，无需手工控制。
   对于堆，应知道系统有一个记录空闲内存地址的链表，当系统收到程序申请时，遍历该链表，寻找第一个空间大于申请空间的堆结点，删    除空闲结点链表中的该结点，并将该结点空间分配给程序（大多数系统会在这块内存空间首地址记录本次分配的大小，这样delete才能正确释放本内存  空间，另外系统会将多余的部分重新放入空闲链表中）。对于栈，只要栈的剩余空间大于所申请空间，系统为程序提供内存，否则报异常提示栈出。

9. .python实现一个generator，不能用yield

   ```
   it = iter([1, 2, 3, 4, 5])
   # 循环:
   while True:
       try:
           # 获得下一个值:
           x = next(it)
       except StopIteration:
           # 遇到StopIteration就退出循环
           break
   ```

10. python实现一个对海量数据做一个计算的优化，因为数据有重复所以不想重复计算（map存下来，用装饰器）

11. CNN前半段和后半段一般参数量哪个比较大

12. C++中push和emplace的区别

    push_back()需要先构造临时对象，再将这个对象拷贝到容器的末尾，而emplace_back()则直接在容器的末尾构造对象，这样就省去了拷贝的过程

13. [双线性差值]([https://blog.csdn.net/qq_37577735/article/details/80041586#%E5%8D%95%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E6%B3%95](https://blog.csdn.net/qq_37577735/article/details/80041586#单线性插值法))



