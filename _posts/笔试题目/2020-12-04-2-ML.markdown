---
layout:     post
title:      " ML笔试题目 "
subtitle:   " ML笔试题目 "
date:       2020-12-04 17:08:00
author:     "DHH"
header-img: "img/bg/leetcodebg.jpg"
catalog: true
tags:
    - 笔试
    - ML
typora-root-url: ..\..
---

> “Yeah It's leetcode problem. ”

## 聚类

1. 关于K-means聚类算法，请回答以下问题：

   1) K-means是有监督聚类还是无监督聚类？（2分）

   2) 写出将N个样本（X=(x1,…,xN)）聚成K类的K-means聚类算法的优化目标函数。（6分）

   3) 请用伪代码写出聚类过程。（8分）

   4) 假设样本特征维度为D，请描述Kmeans算法时间复杂度。（4分）

   **答案：**

   1， 无监督

   2， 优化目标函数：F(X,K) =i=1Kj=1Ni(xj-μi)2

   ​		对于样本xi，计算其分类：

   ![img](https://www.nowcoder.com/equation?tex=c_%7Bi%7D%3Dargmin%7C%7Cx_%7Bi%7D-u_%7Bj%7D%7C%7C%5E%7B2%7D)

   ​	   ci表示类别，uj表示质心

   ​	   那么优化目标函数可以表示为：

   ![img](https://www.nowcoder.com/equation?tex=%5Csum_%7Bi%7D%5E%7BN%7D%7B%7C%7Cx_%7Bi%7D-u_%7Bi%7D%7C%7C%5E%7B2%7D%7D)

   N表示样本量，ui表示ci的质心

   3， 聚类过程：

   ​		初始化：从N个样本中随机选择K个作为初始聚类中心；

   ​		For t=1:T（此处，T为最大迭代次数）

   ​				将N个样本按距离最近原则分配给K个聚类中心；

   ​				迭代更新聚类中心；

   ​				如果达到终止条件，如全部样本归类无变化，或者样本点到聚类中心的平均距离变化率较低，则退出

   4， 时间复杂度：TNKD 其中T为迭代次数、N为样本个数，K为聚类中心数目，D为样本维度

    

## 微积分

### SGD,Momentum,Adagard,Adam原理

 [参考1](https://www.cnblogs.com/jins-note/p/9520089.html) [参考2](https://blog.csdn.net/willduan1/article/details/78070086) [adam](https://www.cnblogs.com/yifdu25/p/8183587.html) 

SGD为随机梯度下降,每一次迭代计算数据集的mini-batch的梯度,然后对参数进行跟新。但是 优化中摆动幅度大，在最优点处容易震荡，要求学习率的设置需要始终，有一个很好的收敛速度同时又不至于摆动幅度太大。Momentum参考了物理中动量的概念,前几次的梯度也会参与到当前的计算中,但是前几轮的梯度叠加在当前计算中会有一定的衰减。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。让梯度的摆动幅度变得更小。

Adagard在训练的过程中可以自动变更学习的速率,设置一个全局的学习率,而实际的学习率与以往的参数模和的开方成反比。对于每个参数，随着其更新的总距离增多，其学习速率也随之变慢。即对于更新频繁步长大的参数，降低其学习率。[提出的原因](https://www.jianshu.com/p/5c4784070d18)  daGrad也是为了解决鞍点和局部最优而出现的，是Rprop的一种改进。Rprop的缺点很明显，梯度容错率过低，如果存在一系列同号的梯度和突然的变号梯度，在所有同号梯度中，梯度会被削弱，而最后的变号梯度会被加强，如果变号梯度是由于计算错误导致的，那么这个错误将会被无限放大（特别是如果算出来的梯度本身就是很大的值的时候）。所以AdaGrad采用了累计平方梯度的思想，也就是用梯度自身的大小来约束梯度。起到的效果是在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。

在分母中累积平方梯度，导致学习率变小并变得无限小，由此导致还未收敛就已经停滞不前，出现early stopping 的现象。

RMSprop对AdaGrad做了一点改进，不再使用单纯的和累计，而是用了指数移动加权平均。这样做的好处就是，首先，可以通过调整![\beta](https://math.jianshu.com/math?formula=%5Cbeta)来决定![S[t]](https://math.jianshu.com/math?formula=S%5Bt%5D)对当前数据的敏感程度，其次由于指数移动平均加权就自带了正则化，所以![S[t]](https://math.jianshu.com/math?formula=S%5Bt%5D)不会一直增大，而是会由加权窗口的数据平均决定，这样就很好地解决了问题。

Adam利用梯度的一阶矩估计(动量)和二阶矩（RMSProp）估计动态调整每个参数的学习率,在经过偏置的校正后,每一次迭代后的学习率都有个确定的范围,使得参数较为平稳。

由于移动指数平均在迭代开始的初期会导致和开始的值有较大的差异，所以我们需要对上面求得的几个值做偏差修正。

另外，在数据比较稀疏的时候，adaptive的方法能得到更好的效果，例如Adagrad，RMSprop, Adam 等。Adam 方法也会比 RMSprop方法收敛的结果要好一些[参考](https://www.jianshu.com/p/d99b83f4c1a6), 所以在实际应用中 ，Adam为最常用的方法，可以比较快地得到一个预估结果。

## 朴素贝叶斯

 3. 最大似然估计和最大后验概率的区别?

    最大似然估计提供了一种给定观察数据来评估模型参数的方法,而最大似然估计中的采样满足所有采样都是独立同分布的假设。最大后验概率是根据经验数据获难以观察量的点估计,与最大似然估计最大的不同是最大后验概率融入了要估计量的先验分布在其中,所以最大后验概率可以看做规则化的最大似然估计。

 4. 概率和似然的区别

    概率是指在给定参数![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552620278389_BC90984DA33C4063A99AC4FA382A724D)的情况下,样本的随机向量X=x的可能性。而似然表示的是在给定样本X=x的情况下,参数![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552620285666_30C645935F00A558745F9D014475B4AC)为真实值的可能性。一般情况,对随机变量的取值用概率表示。而在非贝叶斯统计的情况下,参数为一个实数而不是随机变量,一般用似然来表示。

## PCA

 1. 讲一下PCA

    PCA是比较常见的线性降维方法,通过线性投影将高维数据映射到低维数据中,所期望的是在投影的维度上,新特征自身的方差尽量大,方差越大特征越有效,尽量使产生的新特征间的相关性越小。

    PCA算法的具体操作为对所有的样本进行中心化操作,计算样本的协方差矩阵,然后对协方差矩阵做特征值分解,取最大的n个特征值对应的特征向量构造投影矩阵。

## 集成方法

[GBDT](https://www.cnblogs.com/bnuvincent/p/9693190.html)

[参考2](https://www.cnblogs.com/bnuvincent/p/4905715.html) 

[XGBOOST](https://www.jianshu.com/p/ac1c12f3fba1)

[stacking](https://blog.csdn.net/u010412858/article/details/80785429)

[blending](https://blog.csdn.net/sinat_35821976/article/details/83622594)

[lightgbm](https://www.biaodianfu.com/lightgbm.html) 码了再看

[Shrinkage](https://www.cnblogs.com/peizhe123/p/5086128.html)

常规的机器学习算法问题，比如：XGB和GDBT相比有什么优势？[x先码

- 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
- 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。
- xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
- 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。

- 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。
- xgboost工具支持并行。

LR、XGB、随机森林的原理、优缺点以及应用场景。这类问题几乎逢面必问，不光是腾讯，美团、头条等其它公司也是经常会问，建议大家好好准备一下这类问题；

参考1

LR和XGB算法做特征处理有什么区别？随机森林怎么进行特征选择？等特征处理方面相关的问题；

了解一般的机器学习吗，随机森林、xgboost、gbdt之间有什么区别和联系

## 分类方法

### [SVM](https://blog.csdn.net/ch18328071580/article/details/94168411)

  SVM是一种分类模型，首先，SVM是针对线性可分的情况进行分析的。对于线性不可分的情况，通过使用非线性映射算法将低维输入空间线性不可分的样本转化为高维特征空间，使其线性可分，从而使得在高维特征空间中采用线性算法对样本的非线性特征进行线性分析成为可能。
  它基于结构风险最小化理论，在特征空间中构建**最优**分类面，使得学习器能够得到全局最优化，并且使整个样本空间的期望风险以某个概率满足一定上界。

​	SVM没有使用传统的推导过程，简化了通常的分类和回归等问题；少数的支持向量确定了SVM 的最终决策函数，计算的复杂性取决于支持向量，而不是整个样本空间，这就可以避免“维数灾难”。少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本，而且注定了该方法不但算法简单，而且具有较好的“鲁棒”性。

首先，以线性可分数据为例，假设有m个样本，xi,yi, 其中呢 yi取值为1或-1，我们的目标是，找一个划分超平面，将不同类别的样本分开。划分的超平面很多，我们去找哪一个？直观上看，我们应该找位于两类训练样本“正中间”的超平面。划分超平面可通过如下线性方程来描述：![image-20200229115952823](/img/study/image-20200229115952823.png)
*w*为法向量，决定了超平面的方向；b为位移项，决定了超平面与原点之间的距离。样本空间中任意点x到超平面(w,b)的距离可写为![image-20200229120033413](/img/study/image-20200229120033413.png)假设超平面(w,b)能将训练样本正确分类，即对xiyi,若y=1,则 wx+b>=1, 反之小于，距离超平面最近的这几个训练样本点使上式的等号成立，它们被称为“支持向量，所以两个相反的支持向量之间的间隔为2/||w||, 所以我们的目标就变成了，找到一组w,b，使得间隔最大。我们的上述约束等价于

<img src="/img/study/image-20200229120656133.png" alt="image-20200229120656133" style="zoom:67%;" />，

这就是SVM的基本型。下面来求解模型，上式本身是一个凸二次规划问题，能直接用现成的优化计算包求解，但我们可以有更高效的办法，因为原问题的求解复杂度与特征的维数相关，而转成对偶问题后只与问题的变量个数有关。对式使用**拉格朗日乘子法**可得到其“**对偶问题**”(dual problem)：

<img src="/img/study/image-20200229120807184.png" alt="image-20200229120807184" style="zoom:67%;" />

所以我们的目标就变成了，在alpha>=0时，max(alpha) min（w,b）L。首先对x,b 求偏导数，

<img src="/img/study/image-20200229120941989.png" alt="image-20200229120941989" style="zoom:50%;" />

将第一个式代入L，即可将L(w,b,α)中的w和b消去，再考虑第二个式的约束，就得到对偶问题:

<img src="/img/study/image-20200229121018260.png" alt="image-20200229121018260" style="zoom:50%;" />

求解出对偶问题中的alpha（SMO算法，固定n-2个，先求两个）之后，就可以算出w, b（b的计算是因为对于所有的支撑向量，有y(wx+b)=1,w已知后，便可以求出b）。

从对偶问题解出的alpha_i是拉格朗日乘子，它恰对应着训练样本(xi,yi)。注意到支持向量机最优化问题中有不等式约束，因此上述过程需满足KKT条件，即要求:

<img src="/img/study/image-20200229121543334.png" alt="image-20200229121543334" style="zoom:67%;" />

最后一个条件，对任意训练样本(xi,yi)总有αi=0 或y_{i}f(x_{i})=1

若αi=0则该样本将不会在求和中出现，不会对f(x)有任何影响;

若αi>0,则y_{i}f(x_{i})=1位于最大间隔边界上,是一个支持向量。这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关.

在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分。
缓解该问题的一个办法是允许支持向量机在一-些样本上出错.为此，要引入“软间隔”的概念，在最大化间隔的同时，不满足约束的样本应尽可能少.于是,优化目标可写为:<img src="/img/study/image-20200229121844381.png" alt="image-20200229121844381" style="zoom:50%;" />即，在间隔上加一个损失，允许错分，但是损失应该尽量小。若采用hinge损失，则代价函数变成：<img src="/img/study/image-20200229122051192.png" alt="image-20200229122051192" style="zoom:50%;" />

这就是常用的“软间隔支持向量机”。

当数据是线性不可分的时候，对这样的问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。所以这个时候我们引入了一个映射函数h,原始公式中的x应替换为h(x), 这样替换之后，由于对偶问题中存在xT*x,就变成了求解<img src="/img/study/image-20200229122328923.png" alt="image-20200229122328923" style="zoom: 67%;" />，这是映射之后再特征空间中的内积，由于特征空间维数可能很高，甚至可能是无穷维，所以直接计算很困难，假设存在这样的函数<img src="/img/study/image-20200229122436268.png" alt="image-20200229122436268" style="zoom:67%;" />即xi 与xj 在特征空间的内积等于它们在原始样本空间中通过函数κ(∙,∙ )计算的结果。再次求解对偶函数可得到<img src="/img/study/image-20200229122533445.png" alt="image-20200229122533445" style="zoom:50%;" />

这里的函数κ(∙,∙ )就是“核函数”(kernel function)。显然，若已知合适映射ϕ(∙)的具体形式，则可写出核函数κ(∙,∙ )，但在现实任务中我们通常不知道ϕ(∙)是什么形式，所以，我们通常去寻找一个合适的核函数。核函数的定义是这样的，令χ为输入空间,κ(∙,∙ )是定义在χ×χ上的对称函数,则κ是核函数当且仅当对于任意数据D={x1,x2,…,xm}“核矩阵”(kernel matrix)K总是半正定的。在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，“核函数选择”成为支持向量机的最大变数。

通常，可选择如下核函数，选择性能最优者作为某一问题的核函数：，例如线性核，高斯核。（当样本的特征很多且维数很高时可考虑用SVM的线性核函数。当样本的数量较多,特征较少时,一般手动进行特征的组合再使用SVM的线性核函数。当样本维度不高且数量较少时,且不知道该用什么核函数时一般优先使用高斯核函数,因为高斯核函数为一种局部性较强的核函数,无论对于大样本还是小样本均有较好的性能且相对于多项式核函数有较少的参数。）

### SVR

给定训练样本D={(x1,y1),(x1,y1),……，(xm,ym)},yi∈R,希望学得一个回归模型,使得f(x)与y尽可能接近

假设我们能容忍f(x)与y之间最多有ϵ的偏差,即仅当f(x)与y之间的差别绝对值大于ϵ时才计算损失.



## 基础知识

### [交叉熵](https://blog.csdn.net/tsyccnh/article/details/79163834)

交叉熵基于KL散度计算两个分布之间的距离，将KL散度中的除法展开，可以展开成两项，

<img src="/img/study/image-20200229114632787.png" alt="image-20200229114632787" style="zoom: 67%;" />

等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵：，p的熵即为真实分布的熵，是个常数项所以一般在机器学习中直接用用交叉熵做loss，评估模型。

为什么要用交叉熵做loss函数？[参考](https://zhuanlan.zhihu.com/p/84431551)

在线性回归问题中，常常使用MSE（Mean Squared Error）作为loss函数，如果直接将其扩展到分类问题上，使用sigmoid函数求得最终预测概率，首先 sigmoid函数在边缘处导数很小，很容易出现最开始的时候梯度下降很慢。其次由于对每个位置的特征都做了sigmoid，导致最终的loss函数很可能是非凸的，存在许多局部最优，然而使用log损失就不存在这种问题。MSE对残差大的样例惩罚更大些.比如真实标签分别是(1, 0, 0).模型1的预测标签是(0.8, 0.2, 0),模型2的是(0.9, 0.1, 0).即使输出的标签都是类别0, 但MSE-based算出来模型1的误差是模型2的4倍,而交叉熵-based算出来模型1的误差是模型2的2倍左右.为了弥补模型1在这个样例上的损失,MSE-based需要3个完美预测的样例才能达到和模型2一样的损失,而交叉熵-based只需要一个.实际上,模型输出正确的类别,0.8可能已经是个不错的概率了.



softmax、多个logistic的各自的优势：1、类别数爆炸，2、推了下softmax反向传播的公式，来对比两者的优劣。

### [过拟合]

1. 为什么会发生过拟合

   （1）建模样本抽取错误，包括（但不限于）样本数量太少，抽样方法错误，抽样时没有足够正确考虑业务场景或业务特点，等等导致抽出的样本数据不能有效足够代表业务逻辑或业务场景；
   （2）样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系；
   （3）建模时的“逻辑假设”到了模型应用时已经不能成立了。任何预测模型都是在假设的基础上才可以搭建和应用的，常用的假设包括：假设历史数据可以推测未来，假设业务环节没有发生显著变化，假设建模数据与后来的应用数据是相似的，等等。如果上述假设违反了业务场景的话，根据这些假设搭建的模型当然是无法有效应用的。
   （4）参数太多、模型复杂度高
   （5）决策树模型。如果我们对于决策树的生长没有合理的限制和修剪的话，决策树的自由生长有可能每片叶子里只包含单纯的事件数据(event)或非事件数据（no event），可以想象，这种决策树当然可以完美匹配（拟合）训练数据，但是一旦应用到新的业务真实数据时，效果是一塌糊涂。
   （6）神经网络模型。
   a.由于对样本数据,可能存在隐单元的表示不唯一,即产生的分类的决策面不唯一.随着学习的进行, BP算法使权值可能收敛过于复杂的决策面,并至极致.
   b.权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征.

2. 那如何降低过拟合？

   **权值衰减. 主要应用在神经网络模型中**
   它在每次迭代过程中以某个小因子降低每个权值,这等效于修改E的定义,加入一个与网络权值的总量相应的
   惩罚项,此方法的动机是保持权值较小,避免weight decay,从而使学习过程向着复杂决策面的反方向偏

   **验证数据**
   一个最成功的方法是在训练数据外再为算法提供一套验证数据,应该使用在验证集合上产生最小误差
   的迭代次数,不是总能明显地确定验证集合何时达到最小误差.

   交叉验证：**交叉验证帮助我们确定模型的超参数**。这样训练出来的模型，是经过多次综合比较得出的相对最优模型，在一定程度上可以避免过拟合的问题，这就是为什么我们会说交叉验证可以避免过拟合。

   降低模型复杂度、

   增大数据集、重新采样（改变采样方法等）、重新清洗数据、重新筛选特征。

   dropout、

   正则化、

   集成模型、

   BN

   soft target?

3. 讲下正则化

讲了L0\L1\L2，L1和L2的具体形式和反向传播的时梯度的推导公式、以及它们如何降低过拟合都详细说了。

https://blog.csdn.net/qq_16137569/article/details/81584165

https://www.jianshu.com/p/4bad38fe07e6

4. 泛化误差如何产生，有哪些方法可以减小？

讲了泛化误差的展开式，西瓜书45页，分别说了从bias、var和噪声三个方面减小，并说了相关的方式，以及如何平衡bias和var。

5. 讲下dropout原理。

   Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。

6. dropout训练和测试有什么区别吗？

   https://www.cnblogs.com/ziytong/p/10669239.html

     经过上面屏蔽掉某些神经元，使其激活值为0以后，我们还需要对向量y1……y1000进行缩放，也就是乘以1/(1-p)。如果你在训练的时候，经过置0后，没有对y1……y1000进行缩放（rescale），那么在测试的时候，就需要对权重进行缩放，操作如下。在测试模型阶段   预测模型的时候，每一个神经单元的权重参数要乘以概率p。

7. L1不可导的时候该怎么办[参考](https://blog.csdn.net/pxhdky/article/details/82960659)

   当损失函数不可导,梯度下降不再有效,可以使用坐标轴下降法,梯度下降是沿着当前点的负梯度方向进行参数更新,而坐标轴下降法是沿着坐标轴的方向,假设有m个特征个数,坐标轴下降法进参数更新的时候,先固定m-1个值,然后再求另外一个的局部最优解,从而避免损失函数不可导问题。

   使用Proximal Algorithm近端梯度下降法对L1进行求解,此方法是去优化损失函数上界结果。

   另外 为何L1容易产生稀疏特征，[参考](https://www.cnblogs.com/tianqizhi/p/9703796.html)

8. LASSO回归 [参考](https://blog.csdn.net/pxhdky/article/details/82960659)



## python

1. 多线程和多进程是什么？区别？具体用法？效率提升多少？

   一个程序至少有一个进程,一个进程至少有一个线程. 进程就是一个应用程序在处理机上的一次执行过程，它是一个动态的概念，而线程是进程中的一部分，进程包含多个线程在运行。 多线程可以共享全局变量，多进程不能。多线程中，所有子线程的进程号相同；多进程中，不同的子进程进程号不同。

   Python中由于GIL(全局解释锁：`Global Interpreter Lock`)的存在，在多线程时并没有真正的进行多线程计算。GIL说白了就是伪多线程，一个线程运行其他线程阻塞，使你的多线程代码不是同时执行，而是交替执行。

   所以多线程一般解决IO密集型任务，多进程解决计算密集型任务

   具体用法 一个是thread库，一个是multiprocessing(进程)

    

2. 进程内存互相访问

   共享内存，主要是实现进程间大量数据的传输。所谓共享内存，即在内存中开辟一段特殊的内存空间，多个进程可互斥访问，该内存空间具有自身特有的数据结构。多个进程在使用此共享内存空间时候，必须在进程地址空间与共享内存地址空间之间建立连接，即将共享内存空间挂载到进程中；共享内存是由一个进程开辟，其它任何进程都可以挂载；共享内存并不会随着进程的退出而消失，因此最后不使用此内存空间时，必须要手动删除。

   python的multiprocessing模块也给我们提供了共享内存的操作。

   一般的变量在进程之间是没法进行通讯的，multiprocessing 给我们提供了 Value 和 Array 模块，他们可以在不通的进程中共同使用。父进程中对全局变量的修改不影响子进程中的全局变量，同理，子进程也不影响父进程的。

3. python内存管理，内存池最大？

   当Python中的对象越来越多，占据越来越大的内存，启动垃圾回收(garbage collection)，将没用的对象清除。当Python的某个对象的引用计数降为0时，说明没有任何引用指向该对象，该对象就成为要被回收的垃圾。比如某个新建对象，被分配给某个引用，对象的引用计数变为1。如果引用被删除，对象的引用计数为0，那么该对象就可以被垃圾回收。

   Python中有分为大内存和小内存：（256K为界限分大小内存）

   1、大内存使用malloc进行分配

   2、小内存使用内存池进行分配

4. 如何对一段python代码做加速

5. 讲下python的staticmethod和staticclass：没听过，但面试官谈到了装饰器，我就介绍了装饰器相关的。

   在类A里面的实例方法foo(self, x)，**第一个参数是self**，我们需要有一个A的实例，才可以调用这个函数。当我们需要和类直接进行交互，而不需要和实例进行交互时，类方法是最好的选择。类方法与实例方法类似，静态方法类似普通方法，参数里面不用self。这些方法和类相关，但是又不需要类和实例中的任何信息、属性等等。

   - 子类的实例继承了父类的static_method静态方法，调用该方法，还是调用的父类的方法和类属性。
   - 子类的实例继承了父类的class_method类方法，调用该方法，调用的是子类的方法和子类的类属性。

7. 对浅拷贝和深拷贝有了解吗，讲一下。

   首先深拷贝和浅拷贝都是对象的拷贝，都会生成一个看起来相同的对象，他们本质的区别是拷贝出来的对象的地址是否和原对象一样，也就是地址的复制还是值的复制的区别。深拷贝和浅拷贝需要注意的地方就是可变元素的拷贝：在浅拷贝时，但是新对象里面的可变元素（如列表）的地址和原对象里的可变元素的地址是相同的，所以在新对象或原对象里对这个可变元素做修改时，两个对象是同时改变的，但是深拷贝不会这样，这个是浅拷贝相对于深拷贝最根本的区别。

7. 装饰器

   他们是修改其他函数的功能的函数，使得函数作为参数的时候，代码写起来更加简单，而且复用性更强。

   闭包： 

   　　在一个外函数中定义了一个内函数，内函数里运用了外函数的临时变量，并且外函数的返回值是内函数的引用。这样就构成了一个闭包。

   一般情况下，在我们认知当中，如果一个函数结束，函数的内部所有东西都会释放掉，还给内存，局部变量都会消失。但是闭包是一种特殊情况，如果外函数在结束的时候发现有自己的临时变量将来会在内部函数中用到，就把这个临时变量绑定给了内部函数，然后自己再结束。

8. python里的正则表达式

   ```
   re.match(pattern, string, flags=0)
   ```

9. python可变不可变数据结构。

   可变数据类型：列表list和字典dict。

   不可变数据类型：整型int、浮点型float、字符串型string和元组tuple。

   不可变数据类型的优点就是内存中不管有多少个引用，相同的对象只占用了一块内存，但是它的**缺点**就是当需要**对变量进行运算从而改变变量引用的对象的值时**，由于是不可变的数据类型，所以**必须创建新的对象**，这样就会使得一次次的改变创建了一个个新的对象，不过不再使用的内存会被垃圾回收器回收。

   tuple优点：内存占用低，速度快，可hash。线程安全

10. python lamba与def 定义函数的区别

    被定义的函数是没有名字的；lambda是一个表达式而不是一个语句。它能够出现在Python语法不允许def出现的地方。作为表达式，lambda返回一个值（即一个新的函数），lambda用来编写简单的函数，代码简介，跟python的map()等函数一起使用更简单。

11. 实现opencv中的图像缩放，包括实现双线性插值

12. python中的os.path模块具体用法和场景？



## 代码实现问题

tensorflow 实现拟合三维数据 [参考](https://blog.csdn.net/weixin_41665360/article/details/89671471)

c++实现LR [参考](https://blog.csdn.net/xukaiwen_2016/article/details/70341693)

## C++

1. 虚函数与纯虚函数的区别

   纯虚函数只有定义,没有实现；而虚函数既有定义,也有实现的代码.

   包含纯虚函数的类不能定义其对象（同时含有纯虚函数的类称为抽象类，它不能生成对象）, 而包含虚函数的则可以.

2. c++ static作用

   我知道的：

   修饰全局变量（和global一起时）改变变量的作用域，让它只能在本文件中使用。

   修饰类的成员变量时。就变成静态成员变量，不属于对象，而属于类（初始化一次即可）。不能在类的内部初始化，类中只能声明，定义需要在类外。类外定义时，不用加static关键字，只需要表明类的作用域。

   还有：

   修饰局部变量时，程序一运行起来就给他分配内存，并进行初始化，也是唯一一次初始化。它的生存期为整个源程序，程序结束，它的内存才释放。

   修饰普通函数时，和修饰全局变量一样。函数经过编译产生一个函数符号，被static修饰后，就变为local符号，不参与符号解析，只在本文件中可见。

   

